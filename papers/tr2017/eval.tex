\section{Evaluation}
\label{sec:evaluate}

\MC and \CO have been actively deployed for over a year, and bring the benefits of a 
high level type safe event based languages to thousands of active users. 
In this section we provide a broad, quantitative evalaution of the cost at which 
these benefits are realised. We do this through the generation of a number of micro 
benchmarks that give insights into the performance of \MC and \CO on a selection of 
physical devices. 

The support software of our platform consists of two essential layers. Throughout this
section we break down results into these layers to give an insight into how each layer performs.
(we refer to \emph{\MC wrappers} as \MC throughout this evaluation):

\begin{itemize}
\item \emph{\CO}: the device runtime, against which we write C++ programs;
\item \emph{\MC wrappers}: the C++ and Static TypeScript wrapping for \CO
to make it known to \MC, against which we write Static TypeScript programs;
\end{itemize}

\subsection{Methodology}

To analyse the performance of our solution, we have written a suite of programs to evaluate 
different aspects of \MC and \CO when running on a representative selection of real hardware devices. 
Throughtout, we use the native C++ benchamarks provided by \CO as a baseline, and the Static TypeScript benchmarks show the overhead
added by \MC. 

These programs were written in both C++ and Static TypeScript, and evaluated on the three microcontrollers 
listed in (Table~\ref{table:devices}): The Nordic nRF51 based micro:bit, the Atmel SAMD21 based CPX, and the Atmel Atmega (Uno). 

The Uno is the simplest of these devices, consisting of an 8-bit processor running at 48 MHz, it only has 2kB of RAM (a thousandth of the 
amount a smartphone has in 2017) and 32kB of flash. The Uno has no onboard components other than the microcontroller which supports 
GPIO, Serial, and I2C.

The micro:bit has a 32-bit Cortex M0 clocked at 16MHz, with 16kB RAM and 256kB of flash. It has a number of onboard components 
including a 5x5 LED matrix display, a compass, accelerometer, buttons, GPIO, I2C, Serial, and Bluetooth Low Energy.

The CPX is a 32-bit Cortex-M0+, which offers greater energy efficiency and performance; it clocks at 48 MHz, has 32kB of RAM and 
256kB of flash. On board the CPX are: RGB pixels, an infrared transmitter and receiver, a touch sensor, buttons, accelerometer, speaker, 
microphone, GPIO, I2C, and Serial.

The Uno and micro:bit \MC targets use the untagged strategy, while the CPX target takes the tagged strategy (see Section~\ref{sec:untagged-tagged}).

Our benchmarks are be classified into two types, each with their own methodology:

\begin{enumerate}
    \item \textit{Performance Analysis}: Tests that capture time taken to perform a given operation. here we instrument
    \MC and \CO to perform toggle physical pins on the device at key points in the test code. We them measure the time to
   execute the operation, by using a calibrated oscilloscope observing these pins. This allows us to derive highly accurate real time 
   measurements without biasing the experiment.

    \item \textit{Memory Analysis}: Tests that capture the RAM or FLASH footprint of a certain operation will log a map of memory 
    before executing the operation, execute the operation, and log a map of memory at the end of the operation. 
    A serial terminal is then used to capture the output of these tests.
\end{enumerate}

It is important to note that memory and performance analysis are done in separate runs, 
to ensure logging does not affect time-related measurements.

\subsection{Tight Loop Performance}

To place the performance of \MC in context, we perform a comparative evaluation of \MC against two state-of-the-art 
solutions, using the native C++ as our baseline. The two points of comparison are MicroPython~\cite{MicroPython}, an implementation 
of Python for microcontrollers, and Espruino~\cite{espruinoBook}, an implementation of JavaScript for micrcontrollers. 
For the CPX, a fork of MicroPython known as 'CircuitPython' was used. Both micropython and Espruino adopt virtual machine based
approaches.

To give an indicative general case execution time cost of each solution, we created a simple program that simply 
counts for 0..100000 in a tight loop in each solutions' respective language; 
the results are shown in Table~\ref{table:vm-comparison}. 

For the micro:bit, MicroPython and Espruino are \emph{two or more orders of magnitude slower} than a native \CO program. 
\MC performs only 2x slower. This reflects the simplicity of the STS compiler.
it should be noted that \MC for the CPX uses the tagged approach, which allows for seamless runtime switching to floating point numbers,
resulting in a further 3x slowdown. For both devices, we can observe that \MC outperforms both the VM-based solutions of MicroPython and 
Espruino by at least an order of magnitude. It should also be noted that the Arduino Uno was not used for this comparative experiement as 
MicroPython and Espruino currently require more RAM and FLASH resource than is available in this device.

\begin{table}[]
    \centering
    
    \begin{tabular}{c|c|c|}
    \cline{2-3}
    \multicolumn{1}{l|}{}             & micro:bit       & CPX \\ \cline{2-3}
    \multicolumn{1}{l|}{}             & Time Taken (ms) & Time Taken (ms)   \\ \hline
    \multicolumn{1}{|c|}{\CO}       & 102             & 30.94             \\ \hline
    \multicolumn{1}{|c|}{\MC}    & 216.8           & 227             \\ \hline
    \multicolumn{1}{|c|}{micropython} & 10000           & 5679              \\ \hline
    \multicolumn{1}{|c|}{Espruino}    & 115600          & -                 \\ \hline
    \end{tabular}
    \caption{\label{table:vm-comparison} A comparison between Micropython, Espruino, \MC, and the native C++ of \CO.}
    \end{table}
    

\subsection{Context Switch Performance}

\begin{figure}[ht]
    \includegraphics[width=.7\columnwidth]{images/context-switch.png}
\caption{\label{fig:context-switch}Base context switch profiles per device.}
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=.99\columnwidth]{images/context-vs-stack.png}
\caption{\label{fig:context-vs-stack}Time taken to perform a context switch against stack size.}
\end{figure}

To evaluate the performance of \CON's scheduler we conducted a test that created two fibers and continuously swapped context,
and the time taken to complete the context switch was measured. 
We performed this test in both STS and C++ and the resulting profiles can be seen in Figure~\ref{fig:context-switch}. 
This figure breaks the context switch down into three phases: 
(1) \CO, the time it takes to perform a context switch in \CO; 
(2) Stack, the time taken to page out the \MC stack; and 
(3) \MC, the overhead added by the \MC wrappers. 

From these results we can observe that context switches generally take of order tens of microseconds. 
The cost of \CON's stack paging approach can also be seen as a significant, but not dominant cost. 
The cost of stack paging would of course grow with stack depth.
Figure~\ref{fig:context-vs-stack} therefore profiles the time a context switch takes with an increasing stack size across all three devices in \CO. 
This test is similar to the previous test, however, we placed bytes (in powers of 2) on the stack of each fiber, starting from 64 and finishing at 1024. 
The difference in gradients, and ranges of values can be put down to device capability. 
For instance, the Uno has an 8-bit processor word size, which means more instructions are required to copy the stack, therefore as the stack size increases, so does context switch time. 
The vertical bands indicate typical stack sizes for \MC programs based on a representative set of examples.

%For the uno, the context switch profile is for the native implementation only.

\subsection{Performance of Asynchronous Operations}

\begin{table}[]
\centering
\begin{tabular}{c|c|c|}
    \cline{2-3}
                                                                                                                & \begin{tabular}[c]{@{}c@{}}RAM Overhead\\ (bytes)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Processing Overhead\\ (microseconds)\end{tabular} \\ \hline
    \multicolumn{1}{|c|}{Create a Fiber}                                                                        & 136                                                            & 35.4                                                                         \\ \hline
    \multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Asynchronous \\ Procedure\\ Call\end{tabular}}              & 32                                                             & 4.01                                                                         \\ \hline
    \multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Asynchronous \\ Procedure\\ Call with a Sleep\end{tabular}} & 204                                                            & 32.4                                                                         \\ \hline
    \end{tabular}
\caption{\label{table:time-ram-consumption}RAM consumption and processing time for various asynchronous operations in \CO.}
\end{table}

To gauge the cost of asynchronous operations in \CON, we tested three commonly used code paths, designed to determine the efficiency of \CON's \emph{fork-on-block} Asynchronous Procedure Call (APC) mechanism that underpins all event handlers in \MC and \CO. We measure to RAM and processor cost of: (1) creating a fiber; (2) handling a non-blocking APC call; and (3) handling a blocking APC call. Table~\ref{table:time-ram-consumption} presents our results. Again, the Adafruit CPX device was used for this experiment.

These result highlight the performance gains of the opportunistic fork-on-block mechanism over a naiive approach that would would execute every event handlers in a separate fiber. For non-blocking calls, the best case, this has an small overhead of 32 bytes of RAM and is not processor intensive, versus the worst case, which incurs a large overhead of 204 bytes of RAM and 32.4 microseconds of processor time.

\subsection{Flash Memory Usage}

\begin{table}[]
\centering
\begin{tabular}{c|c|c|c|}
\cline{2-4}
                                                                                                & CPX & micro:bit & Uno  \\ \hline
\multicolumn{1}{|c|}{WA}                                                                       & 20.46 & 12.14     & 7.79 \\ \hline
\multicolumn{1}{|c|}{RT}                                                                       & 29.85 & 34.35     & 13.7 \\ \hline
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}mbed and \\ Supporting Libraries\end{tabular}} & 14.99 & 24.28     & -    \\ \hline
\multicolumn{1}{|c|}{C++ Standard Library}                                                     & 43.14 & 24        & 1.03 \\ \hline
\end{tabular}

\caption{\label{table:flash-consumption}The total flash consumption of code required to support \MC (KB).}
\end{table}

MCU's make use of internal non-volatile FLASH memory to store program code. Table~\ref{table:flash-consumption} shows the per device flash consumption of each software library used in the final \MC binary. To obtain these numbers, we profiled the final map file produced after compilation. The ordering of the table aligns with the composition of the software layer: \MC builds on \CO which builds on the C++ standard library and optionally, mbed.

From the bottom up, the profile of the standard library changes dramatically for each device: The Uno has a very lightweight standard library; the microbit uses 64-bit integer operations (for timers) which requires extra standard library functions; and the CPX requires software floating point operations pulling in more standard library functions.

The size of \CO and \MC scales linearly with the amount of functionality a device has, due to the component oriented nature of \CO and transitively \MC. For instance the Uno has few onboard components when compared to the CPX and micro:bit. The modular composition of \CO allows us to support multiple devices with a variety of feature sets, whilst maintaining the same API at the \MC layer.

\subsection{RAM Memory Usage}

\begin{table}[]
\centering
\begin{tabular}{c|c|c|c|}
\cline{2-4}
                                                                                                & CPX & micro:bit & Uno   \\ \hline
\multicolumn{1}{|c|}{WA}                                                                       & 0.612 & 1.069     & 0.074 \\ \hline
\multicolumn{1}{|c|}{RT}                                                                       & 0.369 & 0.214     & 0.156 \\ \hline
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}mbed and Supporting \\ Libraries\end{tabular}} & 0.312 & 0.923     & -     \\ \hline
\multicolumn{1}{|c|}{C++ Standard Library}                                                     & 0.161 & 0.149     & 0.074 \\ \hline
\end{tabular}
\caption{\label{table:ram-consumption}The total static ram consumption for an \MC binary (KB).}
\end{table}

Table~\ref{table:ram-consumption} shows the per device ram consumption of each software library used in the final \MC binary. To obtain these numbers, we profiled the final map file produced after compilation. At runtime MakeCode also dynamically allocates additional memory: 1.56 kB for the CPX, 560 bytes for the micro:bit, and 644 bytes for the Uno. From the table, we can see that in all cases, the RAM consumption of \MC+\CO is well within the RAM available of each device.

% Add globals maps and profile of listener and fiber.

% The CPX runtime is the largest, as the CPX device has lots of on-board
% components, and this runtime shares much of its code with other SAMD21 \MC targets.
% The compiled C++ runtime is 114k in total, with \CO accounting for 29k, with an
% additional 15k from libmbed. The \MCN-common-packages adds 20k, and also in
% an additional 49k of math support libraries (floating point operations,
% including trigonometry and number printing/parsing).
% As the SAMD21 has 256k of flash, we did not worry about optimizing this runtime for space.

% On the Uno, which has 32k of flash, the much \MC runtime is 8k and \CO is 14k (since
% the Uno only requires basic GPIO, i2c and serial drivers), leaving 10k for user code.

% The TypeScript part of the runtime is 1060 statements on the CPX and 216 on the Uno.
% In our microbenchmarks, 99\% of that runtime is tree-shaken away.

\subsection{\MC Native vs. \MC VM}

Since some MCU's (such as the Arduino Uno) have very little flash memory and quite 
verbose machine code we also built a tiny (around 500 bytes of code) interpreter 
for a simple byte code optimized for code density. This allows us to gain further insights into the 
benefits and drawbacks of compiled vs. interpreted code execution on MCUs.
Table~\ref{table:c-native-vm-stats} describes the difference in performance for the native implementation and the VM implementation with respect to a C++ benchmark. With low computational complexity, the VM performs similarly to native, however, introducing computational tasks causes the VM to operate up to 5 times slower than the native implementation, but provides a 4x improvement in code density.

% Since the Uno is an extremely resource constrained device, the AVR VM was specifically designed for high code density. The interpreter is implemented in assembly, always included with the program, and is around 0.5k. There are about 30 opcodes, some of which can take 1 or 2 byte arguments. There are also a few combined opcodes, representing a sequence of one argument-less opcode, and one with an argument, which improves code density by about 25\%. Opcodes are direct offsets into the code of the interpreter, speeding up execution. They operate on a stack (mainly for function calls) and a special scratch register. There is essentially no stack space overhead compared to native AVR compilation. The speed overhead is around 4x-5x (with respect to native) for computational tasks.

\begin{table}[]
\centering
\begin{tabular}{c|c|c|}
\cline{2-3}
                                    & \begin{tabular}[c]{@{}c@{}}Context Switch Time \\ (microseconds)\end{tabular} & Stack size (bytes) \\ \hline
\multicolumn{1}{|c|}{Uno C++}       & 35.62                                                                         & 16                 \\ \hline
\multicolumn{1}{|c|}{Uno Native WA} & 53.8                                                                          & 34                 \\ \hline
\multicolumn{1}{|c|}{Uno VM WA}     & 62.9                                                                          & 41                 \\ \hline
\end{tabular}

\caption{\label{table:c-native-vm-stats}Context switch performance and stack size for C++, Static TypeScript Native, and virtualised Static TypeScript}
\end{table}

\subsection{Compiling Static TypeScript}

When compiling the Static TypeScript benchmarks and runtime with code shaking we obtain a generated code density of 37.5 bytes per statement on the CPX, 60.8 on AVR native, and 12.3 on AVR VM. This excludes string literals (which don't change the picture significantly), but includes class vtables and number literals if any. Note that ARM and AVR native instructions are 2 bytes each, while AVR VM instructions are between 0.5 and 3 bytes.

When compiling, the entire TypeScript program, including the runtime, is
passed to the TypeScript (TS) language service for parsing. Then, only the remaining part of the program (after code shaking) is compiled to native code.
On a modern laptop, using Node.js, TS parsing and analysis takes about 0.1ms per statement, and \MC compilation to native code takes about 1ms per statement.
While the TS compiler has been optimized for speed, \MCN's native compilation process hasn't been. For example, the CPX TS pass is dominated by compilation of the device runtime and takes about 100ms, whereas the \MC pass typically only includes a small user program and a small bit of the runtime, resulting in less than 100ms. Thus, typical compilation times are under 200ms for typical user programs of 100 lines or less.

% Not really a result, more of a description of design, with an anecdotal comment about speed.

% should be moved further up and described alongside AVR VM vs. Native

% The AVR VM was specifically designed for high code density, since \CO
% leaves less than 10k for TS runtime and user code on the Uno. The interpreter is implemented in assembly, always included with the program, and is around 0.5k.
% There are about 30 opcodes, some of which can take 1 or 2 byte arguments. There are also a few combined opcodes, representing a sequence of one argument-less opcode, and one with an argument, which improves code density by about 25\%. Opcodes are direct offsets into the code of the interpreter, speeding up execution. They operate on a stack (mainly for function calls) and a special scratch register. There is essentially no stack space overhead compared to native AVR compilation. The speed overhead is around 4x-5x (with respect to native) for computational tasks.


%\subsection{Implementation}
% •	\CO (SAMD21 and AVR): base runtime (C++ only)
% •	pxt
% •	pxt-common-packages: C++ and Static TypeScript
% •	pxt-adafruit
% •	pxt-arduino-uno
% •	pxt-monaco, pxt-blockly

%mmoskal [10:10 AM]
%`pxt checkdocs --snippets --re perf --stats`
% [10:11]
% I compile empty sample first twice, to reduce JIT costs
% [10:12]
% also, the first "compile prep" is slightly more costly, since it parses a hex file
